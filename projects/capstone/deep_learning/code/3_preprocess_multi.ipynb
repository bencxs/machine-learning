{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy.io import loadmat\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.tar.gz has already been downloaded!\n",
      "test.tar.gz has already been downloaded!\n",
      "extra.tar.gz has already been downloaded!\n"
     ]
    }
   ],
   "source": [
    "# Download the original SVHN dataset in compressed .tar.gz format.\n",
    "# These contain varying-resolution images (in .png) with multi-digit labels of a number sequence for each image.\n",
    "# Also contains a digitStruct.mat file which stores image name, positions, size, and label of the bounding boxes per digit.\n",
    "url = 'http://ufldl.stanford.edu/housenumbers/'\n",
    "last_percent_reported = None\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "    \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "    slow internet connections. Reports every 5% change in download progress.\n",
    "    \"\"\"\n",
    "    global last_percent_reported\n",
    "    percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "    if last_percent_reported != percent:\n",
    "        if percent % 5 == 0:\n",
    "            sys.stdout.write(\"%s%%\" % percent)\n",
    "            sys.stdout.flush()\n",
    "        else:\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "\n",
    "def maybe_download(filename, force=False):\n",
    "    # Download a file if not present\n",
    "    if force or not os.path.exists(filename):\n",
    "        print('Attempting to download:', filename)\n",
    "        filename, _ = urlretrieve(url + filename, filename, reporthook=download_progress_hook)\n",
    "        print('\\nDownload Complete!')\n",
    "        statinfo = os.stat(filename)\n",
    "        return filename\n",
    "    else:\n",
    "        print(filename + ' has already been downloaded!')\n",
    "\n",
    "train_filename = maybe_download('train.tar.gz')\n",
    "test_filename = maybe_download('test.tar.gz')\n",
    "extra_filename = maybe_download('extra.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train already present - Skipping extraction of train.tar.gz.\n",
      "train\n",
      "test already present - Skipping extraction of test.tar.gz.\n",
      "test\n",
      "extra already present - Skipping extraction of extra.tar.gz.\n",
      "extra\n"
     ]
    }
   ],
   "source": [
    "# Extract downloaded .tar.gz files\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # Remove .tar.gz\n",
    "    if os.path.isdir(root) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "    data_folders = root\n",
    "    print(data_folders)\n",
    "    return data_folders\n",
    "  \n",
    "train_folders = maybe_extract('train.tar.gz')\n",
    "test_folders = maybe_extract('test.tar.gz')\n",
    "extra_folders = maybe_extract('extra.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to extract data from the DigiStruct file into a Python dictionary.\n",
    "# Ref: https://discussions.udacity.com/t/how-to-deal-with-mat-files/160657/5\n",
    "\n",
    "import h5py\n",
    "\n",
    "# The DigitStructFile is just a wrapper around the h5py data.  It basically references \n",
    "#    inf:              The input h5 matlab file\n",
    "#    digitStructName   The h5 ref to all the file names\n",
    "#    digitStructBbox   The h5 ref to all struc data\n",
    "class DigitStructFile:\n",
    "    def __init__(self, inf):\n",
    "        self.inf = h5py.File(inf, 'r')\n",
    "        self.digitStructName = self.inf['digitStruct']['name']\n",
    "        self.digitStructBbox = self.inf['digitStruct']['bbox']\n",
    "\n",
    "# getName returns the 'name' string for for the n(th) digitStruct. \n",
    "    def getName(self,n):\n",
    "        return ''.join([chr(c[0]) for c in self.inf[self.digitStructName[n][0]].value])\n",
    "\n",
    "# bboxHelper handles the coding difference when there is exactly one bbox or an array of bbox. \n",
    "    def bboxHelper(self,attr):\n",
    "        if (len(attr) > 1):\n",
    "            attr = [self.inf[attr.value[j].item()].value[0][0] for j in range(len(attr))]\n",
    "        else:\n",
    "            attr = [attr.value[0][0]]\n",
    "        return attr\n",
    "\n",
    "# getBbox returns a dict of data for the n(th) bbox. \n",
    "    def getBbox(self,n):\n",
    "        bbox = {}\n",
    "        bb = self.digitStructBbox[n].item()\n",
    "        bbox['height'] = self.bboxHelper(self.inf[bb][\"height\"])\n",
    "        bbox['label'] = self.bboxHelper(self.inf[bb][\"label\"])\n",
    "        bbox['left'] = self.bboxHelper(self.inf[bb][\"left\"])\n",
    "        bbox['top'] = self.bboxHelper(self.inf[bb][\"top\"])\n",
    "        bbox['width'] = self.bboxHelper(self.inf[bb][\"width\"])\n",
    "        return bbox\n",
    "\n",
    "    def getDigitStructure(self,n):\n",
    "        s = self.getBbox(n)\n",
    "        s['name']=self.getName(n)\n",
    "        return s\n",
    "\n",
    "# getAllDigitStructure returns all the digitStruct from the input file.     \n",
    "    def getAllDigitStructure(self):\n",
    "        return [self.getDigitStructure(i) for i in range(len(self.digitStructName))]\n",
    "\n",
    "# Return a restructured version of the dataset (one structure by boxed digit).\n",
    "#\n",
    "#   Return a list of such dicts :\n",
    "#      'filename' : filename of the samples\n",
    "#      'boxes' : list of such dicts (one by digit) :\n",
    "#          'label' : 1 to 9 corresponding digits. 10 for digit '0' in image.\n",
    "#          'left', 'top' : position of bounding box\n",
    "#          'width', 'height' : dimension of bounding box\n",
    "#\n",
    "# Note: We may turn this to a generator, if memory issues arise.\n",
    "    def getAllDigitStructure_ByDigit(self):\n",
    "        imgDat = self.getAllDigitStructure()\n",
    "        result = []\n",
    "        structCnt = 1\n",
    "        for i in range(len(imgDat)):\n",
    "            item = { 'filename' : imgDat[i][\"name\"] }\n",
    "            figures = []\n",
    "            for j in range(len(imgDat[i]['height'])):\n",
    "                figure = {}\n",
    "                figure['height'] = imgDat[i]['height'][j]\n",
    "                figure['label']  = imgDat[i]['label'][j]\n",
    "                figure['left']   = imgDat[i]['left'][j]\n",
    "                figure['top']    = imgDat[i]['top'][j]\n",
    "                figure['width']  = imgDat[i]['width'][j]\n",
    "                figures.append(figure)\n",
    "            structCnt = structCnt + 1\n",
    "            item['boxes'] = figures\n",
    "            result.append(item)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from train/digitStruct.mat. This may take a while. Please wait.\n",
      "Extracting data from test/digitStruct.mat. This may take a while. Please wait.\n",
      "Extracting data from extra/digitStruct.mat. This may take a while. Please wait.\n",
      "Complete!\n",
      "{'boxes': [{'width': 81.0, 'top': 77.0, 'label': 1.0, 'left': 246.0, 'height': 219.0}, {'width': 96.0, 'top': 81.0, 'label': 9.0, 'left': 323.0, 'height': 219.0}], 'filename': '1.png'}\n"
     ]
    }
   ],
   "source": [
    "# Run the DigitStructFile function for train, test and extra datasets.\n",
    "\n",
    "def run_DSF(folder_name):\n",
    "    path = os.path.join(folder_name, 'digitStruct.mat')\n",
    "    dsf = DigitStructFile(path)\n",
    "    print(\"Extracting data from %s. This may take a while. Please wait.\" % path)\n",
    "    dataset = dsf.getAllDigitStructure_ByDigit()\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_data = run_DSF('train')\n",
    "test_data = run_DSF('test')\n",
    "extra_data = run_DSF('extra')\n",
    "\n",
    "print(\"Complete!\")\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get data as an array of pixel dimensions for each .png file.\n",
    "from PIL import Image\n",
    "\n",
    "def get_img_size(dataset, folder_name):\n",
    "    img_size = np.ndarray([len(dataset),2])\n",
    "    for i in np.arange(len(dataset)):\n",
    "        filename = dataset[i]['filename']\n",
    "        filepath = os.path.join(folder_name, filename)\n",
    "        imp = Image.open(filepath)\n",
    "        img_size[i, :] = imp.size[:]\n",
    "    return img_size\n",
    "\n",
    "train_imsize = get_img_size(train_data, 'train')\n",
    "test_imsize = get_img_size(test_data, 'test')\n",
    "extra_imsize = get_img_size(extra_data, 'extra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(876.0, 501.0)\n",
      "(25.0, 12.0)\n",
      "(1083.0, 516.0)\n",
      "(31.0, 13.0)\n",
      "(668.0, 415.0)\n",
      "(22.0, 13.0)\n"
     ]
    }
   ],
   "source": [
    "# Print the max and min of the horizontal and vertical pixel length.\n",
    "print(np.amax(train_imsize[:,0]), np.amax(train_imsize[:,1]))\n",
    "print(np.amin(train_imsize[:,0]), np.amin(train_imsize[:,1]))\n",
    "\n",
    "print(np.amax(test_imsize[:,0]), np.amax(test_imsize[:,1]))\n",
    "print(np.amin(test_imsize[:,0]), np.amin(test_imsize[:,1]))\n",
    "\n",
    "print(np.amax(extra_imsize[:,0]), np.amax(extra_imsize[:,1]))\n",
    "print(np.amin(extra_imsize[:,0]), np.amin(extra_imsize[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset from train. This may take a while. Please wait.\n",
      "('#', 29929, 'image has more than 5 digits.')\n",
      "((33402, 32, 32, 1), (33402, 6))\n",
      "Generating dataset from test. This may take a while. Please wait.\n",
      "((13068, 32, 32, 1), (13068, 6))\n",
      "Generating dataset from extra. This may take a while. Please wait.\n",
      "((202353, 32, 32, 1), (202353, 6))\n"
     ]
    }
   ],
   "source": [
    "# Since the dataset comes in varying image sizes, we will crop it into 32x32 images \n",
    "# with care taken to include all digits in the bounding boxes.\n",
    "import PIL.Image as Image\n",
    "\n",
    "def generate_dataset(data, folder):\n",
    "\n",
    "    print(\"Generating dataset from %s. This may take a while. Please wait.\" % folder)\n",
    "    dataset = np.ndarray([len(data),32,32,1], dtype='float32')\n",
    "    labels = np.ones([len(data),6], dtype='int32') * 10 # Encode blank digits as 10\n",
    "    for i in np.arange(len(data)):\n",
    "        filename = data[i]['filename']\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        imp = Image.open(filepath)\n",
    "        boxes = data[i]['boxes']\n",
    "        num_digit = len(boxes)\n",
    "        labels[i,0] = num_digit # Encode index 0 of labels as the number of digits in the sequence. \n",
    "        \n",
    "        top = np.ndarray([num_digit], dtype='float32')\n",
    "        left = np.ndarray([num_digit], dtype='float32')\n",
    "        height = np.ndarray([num_digit], dtype='float32')\n",
    "        width = np.ndarray([num_digit], dtype='float32')\n",
    "        \n",
    "        for j in np.arange(num_digit):\n",
    "            if j < 5: \n",
    "                labels[i,j+1] = boxes[j]['label']\n",
    "                if boxes[j]['label'] == 10: labels[i,j+1] = 0 # Encode index 10 as digit 0\n",
    "            else: print('#',i,'image has more than 5 digits.')\n",
    "            \n",
    "            top[j] = boxes[j]['top']\n",
    "            left[j] = boxes[j]['left']\n",
    "            height[j] = boxes[j]['height']\n",
    "            width[j] = boxes[j]['width']\n",
    "        \n",
    "        im_top = np.amin(top)\n",
    "        im_left = np.amin(left)\n",
    "        im_height = np.amax(top) + height[np.argmax(top)] - im_top\n",
    "        im_width = np.amax(left) + width[np.argmax(left)] - im_left\n",
    "        \n",
    "        im_top = np.floor(im_top - 0.1 * im_height)\n",
    "        im_left = np.floor(im_left - 0.1 * im_width)\n",
    "        im_bottom = np.amin([np.ceil(im_top + 1.2 * im_height), imp.size[1]])\n",
    "        im_right = np.amin([np.ceil(im_left + 1.2 * im_width), imp.size[0]])\n",
    "\n",
    "        im = imp.crop((int(im_left), int(im_top), int(im_right), int(im_bottom))).resize([32,32], Image.ANTIALIAS)\n",
    "        im = np.dot(np.array(im, dtype='float32'), [[0.2989],[0.5870],[0.1140]]) # Convert rgb to grayscale.\n",
    "        mean = np.mean(im, dtype='float32')\n",
    "        std = np.std(im, dtype='float32')\n",
    "        #im = (im - mean) / (1e-8 * std) # Apply GCN. Multiply by 1e-8 to avoid division by zero errors.\n",
    "        if std < 1e-4: std = 1\n",
    "        im = (im - mean) / std # Apply normalization\n",
    "        dataset[i,:,:,:] = im[:,:,:]\n",
    "\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = generate_dataset(train_data, 'train')\n",
    "print(train_dataset.shape, train_labels.shape)\n",
    "\n",
    "test_dataset, test_labels = generate_dataset(test_data, 'test')\n",
    "print(test_dataset.shape, test_labels.shape)\n",
    "\n",
    "extra_dataset, extra_labels = generate_dataset(extra_data, 'extra')\n",
    "print(extra_dataset.shape, extra_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete image indexed 29929 as it contains more than 5 digits.\n",
    "# This is treated as an outlier for our model.\n",
    "train_dataset = np.delete(train_dataset, 29929, axis=0)\n",
    "train_labels = np.delete(train_labels, 29929, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set dimensions: ', (230070, 32, 32, 1), (230070, 6))\n",
      "('Validation set dimensions: ', (5684, 32, 32, 1), (5684, 6))\n",
      "('Testing set dimensions: ', (13068, 32, 32, 1), (13068, 6))\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training, validation and test set.\n",
    "# Ref: https://arxiv.org/pdf/1204.3968.pdf\n",
    "import random\n",
    "\n",
    "random.seed()\n",
    "\n",
    "num_labels = 10\n",
    "valid_ind = []\n",
    "valid_ind_2 = []\n",
    "train_ind = []\n",
    "train_ind_2 = []\n",
    "\n",
    "for i in np.arange(num_labels):\n",
    "    valid_ind.extend(np.where(train_labels[:,1] == (i))[0][:400].tolist())\n",
    "    train_ind.extend(np.where(train_labels[:,1] == (i))[0][400:].tolist())\n",
    "    valid_ind_2.extend(np.where(extra_labels[:,1] == (i))[0][:200].tolist())\n",
    "    train_ind_2.extend(np.where(extra_labels[:,1] == (i))[0][200:].tolist())\n",
    "\n",
    "random.shuffle(valid_ind)\n",
    "random.shuffle(train_ind)\n",
    "random.shuffle(valid_ind_2)\n",
    "random.shuffle(train_ind_2)\n",
    "\n",
    "valid_dataset = np.concatenate((extra_dataset[valid_ind_2,:,:,:], train_dataset[valid_ind,:,:,:]), axis=0)\n",
    "valid_labels = np.concatenate((extra_labels[valid_ind_2,:], train_labels[valid_ind,:]), axis=0)\n",
    "train_dataset = np.concatenate((extra_dataset[train_ind_2,:,:,:], train_dataset[train_ind,:,:,:]), axis=0)\n",
    "train_labels = np.concatenate((extra_labels[train_ind_2,:], train_labels[train_ind,:]), axis=0)\n",
    "\n",
    "print('Training set dimensions: ', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set dimensions: ', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing set dimensions: ', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Compressed pickle size:', 1025147096)\n"
     ]
    }
   ],
   "source": [
    "# Save to pickle file for later reuse.\n",
    "\n",
    "pickle_file = 'SVHN_multi_2.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
